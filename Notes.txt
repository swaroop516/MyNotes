Regression Notes:

Regressions Analysis
Regression analysis is a form of predictive modelling technique which investigates the relationship between a dependent (target) and independent variable (s) (predictor). This technique is used for forecasting, time series modelling and finding the causal effect relationship between the variables.

Types of regression models:
Linear Regression
Logistic Regression
Polynomial Regression
Support Vector Regression
Decision Tree Regression
Random forest Regression
Stepwise Regression
Ridge Regression
Lasso Regression
ElasticNet Regression


There are various kinds of regression techniques available to make predictions. 
These techniques are mostly driven by three metrics 
a.number of independent variables
b.type of dependent variables 
c.shape of regression line

Linear Regression:
Linear Regression establishes a relationship between dependent variable (Y) and one or more independent variables (X) using a best fit straight line (also known as regression line).

Best fit line is bases on Least Square Method.

Important Points:
a.There must be linear relationship between independent and dependent variables
b.Multiple regression suffers from multicollinearity, autocorrelation, heteroskedasticity.
c.Linear Regression is very sensitive to Outliers. It can terribly affect the regression line and eventually the forecasted values.
d.Multicollinearity can increase the variance of the coefficient estimates and make the estimates very sensitive to minor changes in the model. The result is that the coefficient estimates are unstable
e.In case of multiple independent variables, we can go with forward selection, backward elimination and step wise approach for selection of most significant independent variables.

Logistic Regression:
We should use logistic regression when the dependent variable is binary (0/ 1, True/ False, Yes/ No) in nature. Here the value of Y ranges from 0 to 1 and it can represented by following equation.


Important Points:
a.It is widely used for classification problems
b.Logistic regression doesn’t require linear relationship between dependent and independent variables.  It can handle various types of relationships because it applies a non-linear log transformation to the predicted odds ratio
c.To avoid over fitting and under fitting, we should include all significant variables. A good approach to ensure this practice is to use a step wise method to estimate the logistic regression
d.It requires large sample sizes because maximum likelihood estimates are less powerful at low sample sizes than ordinary least square
e.The independent variables should not be correlated with each other i.e. no multi collinearity.  However, we have the options to include interaction effects of categorical variables in the analysis and in the model.
f.If the values of dependent variable is ordinal, then it is called as Ordinal logistic regression
g.If dependent variable is multi class then it is known as Multinomial Logistic regression.


Polynomial Regression:
A regression equation is a polynomial regression equation if the power of independent variable is more than 1. The equation below represents a polynomial equation

y=a+b*x^2

In this regression technique, the best fit line is not a straight line. It is rather a curve that fits into the data points.

Important Points:
a.While there might be a temptation to fit a higher degree polynomial to get lower error, this can result in over-fitting. Always plot the relationships to see the fit and focus on making sure that the curve fits the nature of the problem.
b.Especially look out for curve towards the ends and see whether those shapes and trends make sense. Higher polynomials can end up producing wierd results on extrapolation.



SVM:
“Support Vector Machine” (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems.

SVR will work with both linear and non linear data patterns. It is good with lesser data.
Kerner type will changed for linear and non linear.

gamma: Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. Higher the value of gamma, will try to exact fit the as per training data set i.e. generalization error and cause over-fitting problem.

C: Penalty parameter C of the error term. It also controls the trade off between smooth decision boundary and classifying the training points correctly.


In linear regression we are trying to minimize the error between predicted and data. But in SVR it will try to make sure errors will not cross threshold



#Artificial Neural Networks
ReLu Function can be used only in the hidden layers. 
For output layers we should use a Softmax function for a Classification problem to compute the probabilites for the classes 
and for a regression problem it should simply use a linear function
	