Regression Notes:

Regressions Analysis
Regression analysis is a form of predictive modelling technique which investigates the relationship between a dependent (target) and independent variable (s) (predictor). This technique is used for forecasting, time series modelling and finding the causal effect relationship between the variables.

Types of regression models:
Linear Regression
Logistic Regression
Polynomial Regression
Support Vector Regression
Decision Tree Regression
Random forest Regression
Stepwise Regression
Ridge Regression
Lasso Regression
ElasticNet Regression


There are various kinds of regression techniques available to make predictions. 
These techniques are mostly driven by three metrics 
a.number of independent variables
b.type of dependent variables 
c.shape of regression line

Linear Regression:
Linear Regression establishes a relationship between dependent variable (Y) and one or more independent variables (X) using a best fit straight line (also known as regression line).

Best fit line is bases on Least Square Method.

Important Points:
a.There must be linear relationship between independent and dependent variables
b.Multiple regression suffers from multicollinearity, autocorrelation, heteroskedasticity.
c.Linear Regression is very sensitive to Outliers. It can terribly affect the regression line and eventually the forecasted values.
d.Multicollinearity can increase the variance of the coefficient estimates and make the estimates very sensitive to minor changes in the model. The result is that the coefficient estimates are unstable
e.In case of multiple independent variables, we can go with forward selection, backward elimination and step wise approach for selection of most significant independent variables.

Logistic Regression:
We should use logistic regression when the dependent variable is binary (0/ 1, True/ False, Yes/ No) in nature. Here the value of Y ranges from 0 to 1 and it can represented by following equation.


Important Points:
a.It is widely used for classification problems
b.Logistic regression doesn’t require linear relationship between dependent and independent variables.  It can handle various types of relationships because it applies a non-linear log transformation to the predicted odds ratio
c.To avoid over fitting and under fitting, we should include all significant variables. A good approach to ensure this practice is to use a step wise method to estimate the logistic regression
d.It requires large sample sizes because maximum likelihood estimates are less powerful at low sample sizes than ordinary least square
e.The independent variables should not be correlated with each other i.e. no multi collinearity.  However, we have the options to include interaction effects of categorical variables in the analysis and in the model.
f.If the values of dependent variable is ordinal, then it is called as Ordinal logistic regression
g.If dependent variable is multi class then it is known as Multinomial Logistic regression.


Polynomial Regression:
A regression equation is a polynomial regression equation if the power of independent variable is more than 1. The equation below represents a polynomial equation

y=a+b*x^2

In this regression technique, the best fit line is not a straight line. It is rather a curve that fits into the data points.

Important Points:
a.While there might be a temptation to fit a higher degree polynomial to get lower error, this can result in over-fitting. Always plot the relationships to see the fit and focus on making sure that the curve fits the nature of the problem.
b.Especially look out for curve towards the ends and see whether those shapes and trends make sense. Higher polynomials can end up producing wierd results on extrapolation.



SVM:
“Support Vector Machine” (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems.

SVR will work with both linear and non linear data patterns. It is good with lesser data.
Kerner type will changed for linear and non linear.

gamma: Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. Higher the value of gamma, will try to exact fit the as per training data set i.e. generalization error and cause over-fitting problem.

C: Penalty parameter C of the error term. It also controls the trade off between smooth decision boundary and classifying the training points correctly.


In linear regression we are trying to minimize the error between predicted and data. But in SVR it will try to make sure errors will not cross threshold


Multi Linear Regression:

y=ax1+bx2+cx3+d

If we have a categorical variable in dependent variable list.Convert it to numeric variable based on LabelEncoder.

Note: Assume, we have 500 levels in categorical variables. Then, should we create 500 dummy variables? If you can automate it, very well. Or else, I’d suggest you to first, reduce the levels by using combining methods and then use dummy coding. This would save your time.This method is also known as “One Hot Encoding“.

5 Methods of building model:
1.All in
2.Backward Elimination
3.Forward Elimination
4.Bi-directional Elimination
5.Score Comparision

When to use each
1.All in
When we have prior knowledge of all varaibles
When we have to build models bases on all varaibles as given by buisness

2.Backward Elimination
Step1: Select the significamce level to stay in the model(eg: SL=0.05)
Step2: Fit the full model with all possible predictors
Step3: Consider the predictor with highest p-value.If p>SL, go to step 4 else finish.
Step4: Remove the predictor if its greated than SL
Step5: Fit model without this varible. AFter 5 go back to Step3

3.Forward Elimination
Step1: Select the significamce level to stay in the model(eg: SL=0.05)
Step2: Fit the full model .Select One with Lowest P-Value. Starts with one varaible and all other varaible are added once it quealifies significance level.
Step3: Keep This varaible and fit all possible models with one extra predictor added to the ones you already have.
Step4: Consider the predictor with the lowest P-value. if P<SL , go to STEP3,otherwise go to FIN.

4.Bi-directional Elimination
Step1: Select the significamce level to enter and stay in the model(eg: SLENTER=0.05,SLSTAY=0.05)
Step2: Perform the next step of Forward Selection (new varaibles must have P<SLENTER to enter)
Step3: Perform the All steps of Backward Selection (old varaibles must have P<SLSTAY to enter)
Step4: No new varaible can enter and old varaible cam exit
Step5: Your model is ready

What is P-value?
the p-value or probability value or asymptotic significance is the probability for a given statistical model that, when the null hypothesis is true.

A small p-value (typically ≤ 0.05) indicates strong evidence against the null hypothesis, so you reject the null hypothesis. 
A large p-value (> 0.05) indicates weak evidence against the null hypothesis, so you fail to reject the null hypothesis.

#Artificial Neural Networks
ReLu Function can be used only in the hidden layers. 
For output layers we should use a Softmax function for a Classification problem to compute the probabilites for the classes 
and for a regression problem it should simply use a linear function
	